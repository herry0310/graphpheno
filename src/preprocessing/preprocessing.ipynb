{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--data_path DATA_PATH] [--species SPECIES]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\MY\\AppData\\Roaming\\jupyter\\runtime\\kernel-08bd8733-079e-4146-b717-66ed18f5291c.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001B[1;31mSystemExit\u001B[0m\u001B[1;31m:\u001B[0m 2\n"
     ]
    }
   ],
   "source": [
    "# This file is used to preprocess uniprot and STRING file to get input for Graph2GO model\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from networkx.readwrite import json_graph\n",
    "import networkx as nx\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from scipy import sparse\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from go_anchestor import get_gene_ontology,get_anchestors\n",
    "\n",
    "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "parser.add_argument('--data_path', type=str, default=\"../../data/\", help=\"path storing data.\")\n",
    "parser.add_argument('--species', type=str, default=\"mouse\", help=\"which species to use.\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing uniprot...\n",
      "Loading data...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-2-b449d3beac48>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;31m#### load file\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Loading data...\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 7\u001B[1;33m \u001B[0muniprot_file\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mspecies\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"uniprot-\"\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mspecies\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;34m\".tab\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      8\u001B[0m \u001B[0muniprot\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_table\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0muniprot_file\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0muniprot\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "########## process uniprot ###############\n",
    "print(\"Start processing uniprot...\")\n",
    "\n",
    "#### load file\n",
    "print(\"Loading data...\")\n",
    "uniprot_file = os.path.join(args.data_path, args.species, \"uniprot-\" + args.species + \".tab\")\n",
    "uniprot = pd.read_table(uniprot_file)\n",
    "print(uniprot.shape)\n",
    "\n",
    "\n",
    "#### filtering\n",
    "print(\"filtering...\")\n",
    "# filter by STRING ID occurence\n",
    "uniprot = uniprot[~uniprot['Cross-reference (STRING)'].isna()]\n",
    "uniprot.index = range(uniprot.shape[0])\n",
    "uniprot['Cross-reference (STRING)'] = uniprot['Cross-reference (STRING)'].apply(lambda x:x[:-1])\n",
    "\n",
    "\n",
    "\n",
    "# filter by MGI ID occurence\n",
    "\n",
    "uniprot = uniprot[~uniprot['Cross-reference (MGI)'].isna()]\n",
    "def process_domain(x):\n",
    "    if str(x) == 'nan':\n",
    "        return []\n",
    "    temp = [t.strip() for t in x[:-1].split(\";\")]\n",
    "    # temp = [t.split(\",\")[0] for t in temp]\n",
    "    return temp\n",
    "uniprot['mgi'] = uniprot['Cross-reference (MGI)'].apply(process_domain)\n",
    "for i in uniprot.index:\n",
    "    if len(uniprot['mgi'][i])>=2:\n",
    "         # print(uniprot['mgi'][i])\n",
    "         uniprot.drop(index=i,inplace=True)\n",
    "\n",
    "# filter by sequence length in order to compare with DeepGO\n",
    "uniprot['Length'] = uniprot['Sequence'].apply(len)\n",
    "uniprot = uniprot[ uniprot['Length'] <= 1000 ]\n",
    "\n",
    "# filter by ambiguous amino acid\n",
    "def find_amino_acid(x):\n",
    "    return ('B' in x) | ('O' in x) | ('J' in x) | ('U' in x) | ('X' in x) | ('Z' in x)\n",
    "\n",
    "ambiguous_index = uniprot.loc[uniprot['Sequence'].apply(find_amino_acid)].index\n",
    "uniprot.drop(ambiguous_index, axis=0, inplace=True)\n",
    "uniprot.index = range(len(uniprot))\n",
    "print(\"after filtering:\", uniprot.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "#### obtain GO annotations\n",
    "print(\"obtain GO annotations...\")\n",
    "uniprot['Gene ontology (biological process)'][uniprot['Gene ontology (biological process)'].isna()] = ''\n",
    "uniprot['Gene ontology (cellular component)'][uniprot['Gene ontology (cellular component)'].isna()] = ''\n",
    "uniprot['Gene ontology (molecular function)'][uniprot['Gene ontology (molecular function)'].isna()] = ''\n",
    "\n",
    "def get_GO(x):\n",
    "    pattern = re.compile(r\"GO:\\d+\")\n",
    "    return pattern.findall(x)\n",
    "\n",
    "uniprot['cc'] = uniprot['Gene ontology (cellular component)'].apply(get_GO)\n",
    "uniprot['bp'] = uniprot['Gene ontology (biological process)'].apply(get_GO)\n",
    "uniprot['mf'] = uniprot['Gene ontology (molecular function)'].apply(get_GO)\n",
    "\n",
    "num_cc_before = sum(len(x) for x in uniprot['cc'])\n",
    "num_mf_before = sum(len(x) for x in uniprot['mf'])\n",
    "num_bp_before = sum(len(x) for x in uniprot['bp'])\n",
    "print(\"number of CCs, before enrich\", num_cc_before)\n",
    "print(\"number of MFs, before enrich\", num_mf_before)\n",
    "print(\"number of BPs, before enrich\", num_bp_before)\n",
    "\n",
    "\n",
    "\n",
    "print(\"start enriching go annotations...\")\n",
    "# enrich go terms using ancestors\n",
    "go = get_gene_ontology(os.path.join(args.data_path, \"go-basic.obo\"))\n",
    "BIOLOGICAL_PROCESS = 'GO:0008150'\n",
    "MOLECULAR_FUNCTION = 'GO:0003674'\n",
    "CELLULAR_COMPONENT = 'GO:0005575'\n",
    "\n",
    "new_cc = []\n",
    "new_mf = []\n",
    "new_bp = []\n",
    "\n",
    "for i, row in uniprot.iterrows():\n",
    "    labels = row['cc']\n",
    "    temp = set([])\n",
    "    for x in labels:\n",
    "        temp = temp | get_anchestors(go, x)\n",
    "    temp.discard(CELLULAR_COMPONENT)\n",
    "    new_cc.append(list(temp))\n",
    "\n",
    "    labels = row['mf']\n",
    "    temp = set([])\n",
    "    for x in labels:\n",
    "        temp = temp | get_anchestors(go, x)\n",
    "    temp.discard(MOLECULAR_FUNCTION)\n",
    "    new_mf.append(list(temp))\n",
    "\n",
    "    labels = row['bp']\n",
    "    temp = set([])\n",
    "    for x in labels:\n",
    "        temp = temp | get_anchestors(go, x)\n",
    "    temp.discard(BIOLOGICAL_PROCESS)\n",
    "    new_bp.append(list(temp))\n",
    "\n",
    "uniprot['cc'] = new_cc\n",
    "uniprot['mf'] = new_mf\n",
    "uniprot['bp'] = new_bp\n",
    "\n",
    "num_cc_after = sum(len(x) for x in uniprot['cc'])\n",
    "num_mf_after = sum(len(x) for x in uniprot['mf'])\n",
    "num_bp_after = sum(len(x) for x in uniprot['bp'])\n",
    "print(\"number of CCs, after enrich\", num_cc_after)\n",
    "print(\"number of MFs, after enrich\", num_mf_after)\n",
    "print(\"number of BPs, after enrich\", num_bp_after)\n",
    "\n",
    "\n",
    "\n",
    "#### filter GO terms by the number of occurence\n",
    "print(\"filter GO terms by the number of occurence...\")\n",
    "# filter GO by the number of occurence\n",
    "mf_items = [item for sublist in uniprot['mf'] for item in sublist]\n",
    "mf_unique_elements, mf_counts_elements = np.unique(mf_items, return_counts=True)\n",
    "bp_items = [item for sublist in uniprot['bp'] for item in sublist]\n",
    "bp_unique_elements, bp_counts_elements = np.unique(bp_items, return_counts=True)\n",
    "cc_items = [item for sublist in uniprot['cc'] for item in sublist]\n",
    "cc_unique_elements, cc_counts_elements = np.unique(cc_items, return_counts=True)\n",
    "\n",
    "mf_list = mf_unique_elements[np.where(mf_counts_elements >= 50)]\n",
    "cc_list = cc_unique_elements[np.where(cc_counts_elements >= 50)]\n",
    "bp_list = bp_unique_elements[np.where(bp_counts_elements >= 250)]\n",
    "\n",
    "temp_mf = uniprot['mf'].apply(lambda x: list(set(x) & set(mf_list)))\n",
    "uniprot['filter_mf'] = temp_mf\n",
    "temp_cc = uniprot['cc'].apply(lambda x: list(set(x) & set(cc_list)))\n",
    "uniprot['filter_cc'] = temp_cc\n",
    "temp_bp = uniprot['bp'].apply(lambda x: list(set(x) & set(bp_list)))\n",
    "uniprot['filter_bp'] = temp_bp\n",
    "\n",
    "# write out filtered ontology lists\n",
    "def write_go_list(ontology,ll):\n",
    "    filename = os.path.join(args.data_path, args.species, ontology+\"_list.txt\")\n",
    "    with open(filename,'w') as f:\n",
    "        for x in ll:\n",
    "            f.write(x + '\\n')\n",
    "print(\"writing go term list...\")\n",
    "write_go_list('cc',cc_list)\n",
    "write_go_list('mf',mf_list)\n",
    "write_go_list('bp',bp_list)\n",
    "\n",
    "\n",
    "\n",
    "#### encode GO terms\n",
    "print(\"encoding GO terms...\")\n",
    "mf_dict = dict(zip(list(mf_list),range(len(mf_list))))\n",
    "cc_dict = dict(zip(list(cc_list),range(len(cc_list))))\n",
    "bp_dict = dict(zip(list(bp_list),range(len(bp_list))))\n",
    "mf_encoding = [[0]*len(mf_dict) for i in range(len(uniprot))]\n",
    "cc_encoding = [[0]*len(cc_dict) for i in range(len(uniprot))]\n",
    "bp_encoding = [[0]*len(bp_dict) for i in range(len(uniprot))]\n",
    "\n",
    "for i,row in uniprot.iterrows():\n",
    "    for x in row['filter_mf']:\n",
    "        mf_encoding[i][ mf_dict[x] ] = 1\n",
    "    for x in row['filter_cc']:\n",
    "        cc_encoding[i][ cc_dict[x] ] = 1\n",
    "    for x in row['filter_bp']:\n",
    "        bp_encoding[i][ bp_dict[x] ] = 1\n",
    "\n",
    "uniprot['cc_label'] = cc_encoding\n",
    "uniprot['mf_label'] = mf_encoding\n",
    "uniprot['bp_label'] = bp_encoding\n",
    "\n",
    "uniprot.drop(columns=['mf','cc','bp','Gene ontology (biological process)',\n",
    "                      'Gene ontology (cellular component)',\n",
    "                      'Gene ontology (molecular function)'],inplace=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#### encode amino acid sequence using CT\n",
    "print(\"encode amino acid sequence using CT...\")\n",
    "def CT(sequence):\n",
    "    classMap = {'G':'1','A':'1','V':'1','L':'2','I':'2','F':'2','P':'2',\n",
    "            'Y':'3','M':'3','T':'3','S':'3','H':'4','N':'4','Q':'4','W':'4',\n",
    "            'R':'5','K':'5','D':'6','E':'6','C':'7'}\n",
    "\n",
    "    seq = ''.join([classMap[x] for x in sequence])\n",
    "    length = len(seq)\n",
    "    coding = np.zeros(343,dtype=np.int)\n",
    "    for i in range(length-2):\n",
    "        index = int(seq[i]) + (int(seq[i+1])-1)*7 + (int(seq[i+2])-1)*49 - 1\n",
    "        coding[index] = coding[index] + 1\n",
    "    return coding\n",
    "\n",
    "CT_list = []\n",
    "for seq in uniprot['Sequence'].values:\n",
    "    CT_list.append(CT(seq))\n",
    "uniprot['CT'] = CT_list\n",
    "\n",
    "\n",
    "#### encode subcellular location\n",
    "print(\"encode subcellular location...\")\n",
    "\n",
    "def process_sub_loc(x):\n",
    "    if str(x) == 'nan':\n",
    "        return []\n",
    "    x = x[22:-1]\n",
    "    # check if exists \"Note=\"\n",
    "    pos = x.find(\"Note=\")\n",
    "    if pos != -1:\n",
    "        x = x[:(pos-2)]\n",
    "    temp = [t.strip() for t in x.split(\".\")]\n",
    "    temp = [t.split(\";\")[0] for t in temp]\n",
    "    temp = [t.split(\"{\")[0].strip() for t in temp]\n",
    "    temp = [x for x in temp if '}' not in x and x != '']\n",
    "    return temp\n",
    "\n",
    "uniprot['Sub_cell_loc'] = uniprot['Subcellular location [CC]'].apply(process_sub_loc)\n",
    "items = [item for sublist in uniprot['Sub_cell_loc'] for item in sublist]\n",
    "items = np.unique(items)\n",
    "sub_mapping = dict(zip(list(items),range(len(items))))\n",
    "sub_encoding = [[0]*len(items) for i in range(len(uniprot))]\n",
    "for i,row in uniprot.iterrows():\n",
    "    for loc in row['Sub_cell_loc']:\n",
    "        sub_encoding[i][ sub_mapping[loc] ] = 1\n",
    "uniprot['Sub_cell_loc_encoding'] = sub_encoding\n",
    "uniprot.drop(['Subcellular location [CC]'],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "#### encode protein domains\n",
    "print(\"encode protein domains...\")\n",
    "\n",
    "def process_domain(x):\n",
    "    if str(x) == 'nan':\n",
    "        return []\n",
    "    temp = [t.strip() for t in x[:-1].split(\";\")]\n",
    "    return temp\n",
    "\n",
    "uniprot['protein-domain'] = uniprot['Cross-reference (Pfam)'].apply(process_domain)\n",
    "items = [item for sublist in uniprot['protein-domain'] for item in sublist]\n",
    "unique_elements, counts_elements = np.unique(items, return_counts=True)\n",
    "items = unique_elements[np.where(counts_elements > 5)]\n",
    "pro_mapping = dict(zip(list(items),range(len(items))))\n",
    "pro_encoding = [[0]*len(items) for i in range(len(uniprot))]\n",
    "\n",
    "for i,row in uniprot.iterrows():\n",
    "    for fam in row['protein-domain']:\n",
    "        if fam in pro_mapping:\n",
    "            pro_encoding[i][ pro_mapping[fam] ] = 1\n",
    "\n",
    "uniprot['Pro_domain_encoding'] = pro_encoding\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#### wirte files\n",
    "print(\"write files...\")\n",
    "uniprot.to_pickle(os.path.join(args.data_path, args.species, \"features.pkl\"))\n",
    "uniprot[['Entry','Gene names','Cross-reference (STRING)']].to_csv(os.path.join(args.data_path,args.species,\"gene_list.csv\"),\n",
    "                                                                 index_label='ID')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "MGI_file = os.path.join(args.data_path, args.species, \"MGI_Phenotype.txt\")\n",
    "mgi = pd.read_table(MGI_file)\n",
    "def get_GNI(x):\n",
    "    pattern = re.compile(r\"\\d+\")\n",
    "    return pattern.findall(x)\n",
    "mgi['mgi'] = mgi['MGI Marker Accession ID'].apply(get_GNI)\n",
    "gene_mgi = pd.read_csv(os.path.join(args.data_path,args.species,\"gene_mgi.csv\"))\n",
    "mgi_id = mgi[mgi['mgi'].isin(gene_mgi['mgi'].values)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#################################\n",
    "######## process PPIs ###########\n",
    "\n",
    "print(\"Start processing PPIs...\")\n",
    "\n",
    "string_file = os.path.join(args.data_path, args.species, \"string-mouse.txt\")\n",
    "string = pd.read_table(string_file, delimiter=\" \")\n",
    "gene_list = pd.read_csv(os.path.join(args.data_path,args.species,\"gene_list.csv\"))\n",
    "\n",
    "# filter by uniprot\n",
    "string = string[string['protein1'].isin(gene_list['Cross-reference (STRING)'].values)]\n",
    "string = string[string['protein2'].isin(gene_list['Cross-reference (STRING)'].values)]\n",
    "\n",
    "# map names to indexs\n",
    "id_mapping = dict(zip(list(gene_list['Cross-reference (STRING)'].values),\n",
    "                     list(gene_list['ID'].values)))\n",
    "string['protein1_id'] = string['protein1'].apply(lambda x:id_mapping[x])\n",
    "string['protein2_id'] = string['protein2'].apply(lambda x:id_mapping[x])\n",
    "\n",
    "subnetwork = string[['protein1_id','protein2_id','combined_score']]\n",
    "subnetwork['combined_score'] = subnetwork['combined_score']/1000.0\n",
    "subnetwork.to_csv(os.path.join(args.data_path, args.species, \"networks/ppi.txt\"), index=False, header=False, sep=\"\\t\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###################################\n",
    "######## process similarity #######\n",
    "\n",
    "print(\"Start processing similarity...\")\n",
    "\n",
    "def write_fasta(feats):\n",
    "    filename = os.path.join(args.data_path, args.species, \"blast/uniprot_seq.fas\")\n",
    "    with open(filename, \"w\") as f:\n",
    "        for i,row in feats.iterrows():\n",
    "            f.write(\">\" + row['Entry'] + \"\\n\")\n",
    "            f.write(row['Sequence'] + \"\\n\")\n",
    "\n",
    "write_fasta(uniprot)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}